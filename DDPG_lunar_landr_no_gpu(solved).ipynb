{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDPG_lunar_landr-no gpu.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "urEHsW7fpKX4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea7ce49-4ea5-4ec6-eb04-27f8400c4005"
      },
      "source": [
        "#!pip install box2d\n",
        "#!pip install wandb -q\n",
        "\n",
        "import wandb\n",
        "import torch     \n",
        "from torch import Tensor        \n",
        "import torch.autograd as autograd           \n",
        "import torch.nn as nn                   \n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init        \n",
        "import torch.optim as optim      \n",
        "from torch.distributions import Categorical         \n",
        "import gym\n",
        "import numpy as np\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from collections import deque\n",
        "import collections, itertools"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "ulyfXJuA7Sxa",
        "outputId": "86cadb20-42d6-4bbc-fff0-03a7f46c4c4d"
      },
      "source": [
        "wandb.login()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
            "wandb: Currently logged in as: olayemiy (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oz4sM6_U6JU"
      },
      "source": [
        "def init_fanin(tensor): #check for fan-in DDPG paper 7. experiment details\r\n",
        "  fanin = tensor.size(1)\r\n",
        "  v = 1.0 / np.sqrt(fanin)\r\n",
        "  init.uniform_(tensor, -v, v)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNHbnNtzpUpL"
      },
      "source": [
        " class DDPG_AC(nn.Module):\n",
        "   def __init__(self, obs_size, num_actions, linear_dim1, linear_dim2): \n",
        "     super(DDPG_AC,self).__init__()   \n",
        "     self.actor_fc1= nn.Linear(obs_size, linear_dim1)\n",
        "     init_fanin(self.actor_fc1.weight)\n",
        "     self.bn1 = nn.LayerNorm(linear_dim1)\n",
        "\n",
        "     self.actor_fc2 = nn.Linear(linear_dim1,linear_dim2)\n",
        "     init_fanin(self.actor_fc2.weight)\n",
        "     self.bn2 = nn.LayerNorm(linear_dim2)\n",
        "\n",
        "     self.actor_fc3 = nn.Linear(linear_dim2,num_actions)\n",
        "     init.uniform_(self.actor_fc3.weight, -3e-3, 3e-3)\n",
        "     init.uniform_(self.actor_fc3.bias, -3e-3, 3e-3)\n",
        "\n",
        "   def forward(self, obs):\n",
        "    actor = F.relu(self.bn1(self.actor_fc1(obs)))\n",
        "    actor = F.relu(self.bn2(self.actor_fc2(actor)))\n",
        "    actor = torch.tanh(self.actor_fc3(actor))\n",
        "\n",
        "    return actor\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJPIYbmQkyZk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4f3d42f9-9fb3-4252-98b5-2cdba174f87a"
      },
      "source": [
        "'''\n",
        "env = gym.make('LunarLanderContinuous-v2')\n",
        "obs = env.reset()\n",
        "print(obs)\n",
        "test = DDPG_AC(obs.shape[0],env.action_space.shape[0], 25).float() \n",
        "print(obs.shape)\n",
        "print(env.action_space.shape)\n",
        "print(test.forward(torch.tensor(obs).float()))\n",
        "'''\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nenv = gym.make('LunarLanderContinuous-v2')\\nobs = env.reset()\\nprint(obs)\\ntest = DDPG_AC(obs.shape[0],env.action_space.shape[0], 25).float() \\nprint(obs.shape)\\nprint(env.action_space.shape)\\nprint(test.forward(torch.tensor(obs).float()))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FuiC4SSFr85"
      },
      "source": [
        "class DDPG_CR(nn.Module):\n",
        "  def __init__(self, obs_size, num_actions, linear_dim1, linear_dim2): #linear_dim1 = 400\n",
        "    super(DDPG_CR,self).__init__()\n",
        "\n",
        "    self.critic_fc1 = nn.Linear(obs_size , linear_dim1)\n",
        "    init_fanin(self.critic_fc1.weight)\n",
        "    self.bn1 = nn.LayerNorm(linear_dim1)\n",
        "\n",
        "    self.critic_fc2 = nn.Linear(linear_dim1 + num_actions, linear_dim2) #inserting actions at second layer now\n",
        "    init_fanin(self.critic_fc2.weight)\n",
        "    self.bn2 = nn.LayerNorm(linear_dim2)\n",
        "\n",
        "    self.critic_fc3 = nn.Linear(linear_dim2, 1)\n",
        "    init.uniform_(self.critic_fc3.weight, -3e-3, 3e-3)\n",
        "    init.uniform_(self.critic_fc3.bias, -3e-3, 3e-3)\n",
        "\n",
        "  def forward(self, obs, action):\n",
        "    critic = F.relu(self.bn1(self.critic_fc1(obs)))\n",
        "    critic = torch.cat((critic, action), -1)\n",
        "    critic = F.relu(self.bn2(self.critic_fc2(critic)))\n",
        "    critic = self.critic_fc3(critic)\n",
        "\n",
        "    return critic #returns value for critic"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zu2RGUbtuE4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8c2b99fd-6f9d-49f2-bc58-622d02421b89"
      },
      "source": [
        "'''\n",
        "env = gym.make('LunarLanderContinuous-v2')\n",
        "obs = env.reset()\n",
        "test = DDPG_AC(obs.shape[0],env.action_space.shape[0], 25) \n",
        "cr = DDPG_CR(obs.shape[0],env.action_space.shape[0], 25) \n",
        "obs = torch.tensor(obs)\n",
        "action = test.forward(obs)\n",
        "print(obs.shape, action.shape)\n",
        "print(cr.forward(obs, action))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nenv = gym.make('LunarLanderContinuous-v2')\\nobs = env.reset()\\ntest = DDPG_AC(obs.shape[0],env.action_space.shape[0], 25) \\ncr = DDPG_CR(obs.shape[0],env.action_space.shape[0], 25) \\nobs = torch.tensor(obs)\\naction = test.forward(obs)\\nprint(obs.shape, action.shape)\\nprint(cr.forward(obs, action))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL3HU7pI6DhU"
      },
      "source": [
        "class OUActionNoise(object):\n",
        "    def __init__(self, mu, sigma=0.15, theta=.2, dt=1e-2, x0=None):\n",
        "        self.theta = theta\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.dt = dt\n",
        "        self.x0 = x0\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):#kinda like init but runs when you do noise().. if we created a noise object\n",
        "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
        "            self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
        "\n",
        "    def __repr__(self):#string representation of object\n",
        "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmUBidw1bCN2"
      },
      "source": [
        "class Agent():\n",
        "  def __init__(self, ENV_NAME='LunarLanderContinuous-v2', LINEAR_SIZE1=400, LINEAR_SIZE2=300, REPLAY_SIZE=int(1e6), BATCH_SIZE=64, TAU=0.001, AC_LR=0.001, CR_LR=0.001): #increased replay_size to 1 milli form 10,000\n",
        "    self.GAMMA = 0.99\n",
        "    self.TAU = TAU   # that T looking symbol for updating target network  \n",
        "    self.batch_size = BATCH_SIZE\n",
        "\n",
        "    self.env = gym.make(ENV_NAME)\n",
        "    self.obs = self.env.reset()\n",
        "\n",
        "    self.noise = OUActionNoise(mu=np.zeros(self.env.action_space.shape[0]))\n",
        "\n",
        "    self.net_ac = DDPG_AC(self.obs.shape[0], self.env.action_space.shape[0], LINEAR_SIZE1, LINEAR_SIZE2).float()\n",
        "    self.net_cr = DDPG_CR(self.obs.shape[0], self.env.action_space.shape[0], LINEAR_SIZE1, LINEAR_SIZE2).float()\n",
        "\n",
        "    self.target_net_ac = DDPG_AC(self.obs.shape[0], self.env.action_space.shape[0], LINEAR_SIZE1, LINEAR_SIZE2).float()\n",
        "    self.target_net_cr = DDPG_CR(self.obs.shape[0], self.env.action_space.shape[0], LINEAR_SIZE1, LINEAR_SIZE2).float()\n",
        "\n",
        "    self.target_net_ac.load_state_dict(self.net_ac.state_dict())\n",
        "    self.target_net_cr.load_state_dict(self.net_cr.state_dict())\n",
        "\n",
        "    self.optimizer_ac = optim.Adam(self.net_ac.parameters(), lr=AC_LR)\n",
        "    self.optimizer_cr = optim.Adam(self.net_cr.parameters(), lr=CR_LR)\n",
        "\n",
        "    self.Experience = collections.namedtuple('Experience',['state', 'action', 'reward', 'done', 'state1'])\n",
        "    self.memory = collections.deque(maxlen = REPLAY_SIZE)\n",
        "    self.max_replay = REPLAY_SIZE\n",
        "\n",
        "  \n",
        "  def init_replay_buffer(self):\n",
        "    for i in range(5000): \n",
        "      action = self.env.action_space.sample()\n",
        "      obs1, reward, done, info = self.env.step(action)\n",
        "      experience = self.Experience(self.obs, action, reward, done, obs1)\n",
        "      self.memory.append(experience)\n",
        "      self.obs = self.env.reset() if done else obs1\n",
        "      \n",
        "      if(i>=1000 and i%50==0):\n",
        "        states, actions, rewards, dones, state1s = self.get_batch()\n",
        "        critic_loss, actor_loss = self.calc_loss_update(states, actions, rewards, dones, state1s)\n",
        "        self.update_target()\n",
        "        \n",
        "\n",
        "  def act(self, episode_step_count, max_episode_steps):\n",
        "    self.obs = torch.tensor(self.obs)\n",
        "    action = self.net_ac.forward(self.obs.float())\n",
        "\n",
        "    self.action_log = action[0].item()\n",
        "\n",
        "    action = action + torch.Tensor(self.noise()) \n",
        "\n",
        "    self.action_noised_log = action[0].item()\n",
        "\n",
        "    obs1, reward, done, info = self.env.step(action.detach().numpy())\n",
        "\n",
        "    if(episode_step_count >= max_episode_steps): #this way we reset the obs and take care of the reward func calculation\n",
        "      done = True\n",
        "    \n",
        "    experience = self.Experience(self.obs.detach().numpy(), action.detach().numpy(), reward, done, obs1)\n",
        "    \n",
        "    self.obs = self.env.reset() if done else obs1\n",
        "\n",
        "    return experience, reward, done  #returning done to count number of episodes\n",
        "\n",
        "  def store_in_batch(self, experience):\n",
        "    self.memory.append(experience)\n",
        "\n",
        "  def get_batch(self):\n",
        "    random_indexes = np.random.choice(len(self.memory), self.batch_size, replace = False) \n",
        "    \n",
        "    states, actions, rewards, dones, state1s = zip(* [self.memory[index] for index in random_indexes]) # returns a list for each category\n",
        "\n",
        "    states = torch.Tensor(np.array(states))           \n",
        "    actions = torch.Tensor(np.array(actions))               \n",
        "    rewards = torch.Tensor(rewards)      \n",
        "    dones = torch.Tensor(dones)       \n",
        "    state1s = torch.Tensor(state1s)\n",
        "\n",
        "    return states, actions, rewards, dones, state1s\n",
        "\n",
        "  def calc_loss_update(self, states, actions, rewards, dones, state1s):\n",
        "    target_actor = self.target_net_ac(state1s.float())\n",
        "    target_critic = self.target_net_cr(state1s.float(), target_actor.detach())\n",
        "    target_critic = torch.squeeze(target_critic)\n",
        "\n",
        "    critic = self.net_cr(states.float(), actions.float())\n",
        "    critic = torch.squeeze(critic)\n",
        "    \n",
        "    #y = rewards + (self.GAMMA * target_critic * dones) \n",
        "    y = torch.empty(self.batch_size)\n",
        "    for i in range(self.batch_size):\n",
        "      y[i] = rewards[i] + (self.GAMMA * target_critic[i] * (1-dones[i]))\n",
        "\n",
        "    #Critic Loss\n",
        "    critic_loss = F.mse_loss(y.detach(), critic.float())\n",
        "    \n",
        "    self.optimizer_cr.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    self.optimizer_cr.step()\n",
        "\n",
        "    #Actor Loss\n",
        "    actor_loss = -self.net_cr(states,self.net_ac(states)).mean()\n",
        "\n",
        "    self.optimizer_ac.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    self.optimizer_ac.step()\n",
        "\n",
        "    return critic_loss, actor_loss\n",
        "    \n",
        "  def update_target(self):\n",
        "    for target_param, param in zip(self.target_net_ac.parameters(), self.net_ac.parameters()):\n",
        "      target_param.data.copy_(self.TAU * param.data + (1 - self.TAU) * target_param.data)\n",
        "\n",
        "    for target_param, param in zip(self.target_net_cr.parameters(), self.net_cr.parameters()):\n",
        "      target_param.data.copy_(self.TAU * param.data + (1 - self.TAU) * target_param.data)\n",
        "\n",
        "  def close_agent(self):\n",
        "    self.env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DKsCaUWcSEh"
      },
      "source": [
        "def train():\n",
        "  agent = Agent()\n",
        "  agent.init_replay_buffer()\n",
        "  episode_count = 0\n",
        "  avg_reward = 0\n",
        "  avg_100 =  collections.deque(maxlen = 100)\n",
        "  episode_step_count = 0\n",
        "  max_episode_steps = 1000\n",
        "  \n",
        "  wandb.init(project=\"ddpg\")\n",
        "  wandb.watch(agent.net_ac, log =\"all\")\n",
        "  wandb.watch(agent.net_cr, log =\"all\")\n",
        "  \n",
        "  while (episode_count < 7000): \n",
        "    with torch.autograd.set_detect_anomaly(True):\n",
        "      experience, reward, done = agent.act(episode_step_count, max_episode_steps)\n",
        "      agent.store_in_batch(experience)\n",
        "      states, actions, rewards, dones, state1s = agent.get_batch()\n",
        "      critic_loss, actor_loss = agent.calc_loss_update(states, actions, rewards, dones, state1s)\n",
        "\n",
        "      agent.update_target()\n",
        "\n",
        "      avg_reward+=reward\n",
        "      episode_step_count += 1\n",
        "\n",
        "      if done:\n",
        "        avg_100.append(avg_reward)\n",
        "        trail = (sum(avg_100)/len(avg_100))\n",
        "        \n",
        "        wandb.log({\"Reward\": avg_reward,\n",
        "                   \"Average 100 Rewards\": trail,\n",
        "                  \"Actor Loss\": actor_loss,\n",
        "                  \"Critic Loss\": critic_loss,\n",
        "                  \"Action\": agent.action_log,\n",
        "                  \"Noise + Action\": agent.action_noised_log,\n",
        "                  \"Episode Step Count\": episode_step_count,\n",
        "                  \"Replay Memory Size\": len(agent.memory)})\n",
        "               \n",
        "        if(trail >= 200):\n",
        "          torch.save(agent.net_ac.state_dict(),'lunar_saved.pth')\n",
        "          agent.close_agent()\n",
        "          break\n",
        "        if(episode_count%20==0):\n",
        "          torch.save(agent.net_ac.state_dict(),'lunar_saved.pth')\n",
        "\n",
        "        episode_count+=1\n",
        "        episode_step_count = 0\n",
        "        print('episode: ',episode_count, 'reward: ', avg_reward)\n",
        "\n",
        "        avg_reward = 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3TUHVFHh755"
      },
      "source": [
        "\n",
        "'''\n",
        "#use to test model on computer\n",
        "import gym\n",
        "env = gym.make('LunarLanderContinuous-v2')\n",
        "observation = env.reset()\n",
        "net = DDPG_AC(observation.shape[0], env.action_space.shape[0], 400, 300).float()\n",
        "net.load_state_dict(torch.load('lunar_saved.pth'))\n",
        "net.eval()\n",
        "for i_episode in range(10):\n",
        "    observation = env.reset()\n",
        "    done = False\n",
        "    t = 0\n",
        "    total_r = 0\n",
        "    while not done:\n",
        "        env.render()\n",
        "        action = net(torch.tensor(observation).float())##\n",
        "        observation, reward, done, info = env.step(action.detach().numpy())\n",
        "        total_r+=reward\n",
        "        if done:\n",
        "            print(\"Episode finished after {} timesteps.\".format(t+1), \"Reward: \",total_r)\n",
        "            break\n",
        "        t+=1\n",
        "env.close()\n",
        "'''"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished after 342 timesteps .Reward:  205.76836661123698\n",
            "Episode finished after 1000 timesteps .Reward:  -45.21964558978234\n",
            "Episode finished after 219 timesteps .Reward:  193.13217254422935\n",
            "Episode finished after 261 timesteps .Reward:  203.09862346930754\n",
            "Episode finished after 453 timesteps .Reward:  251.6061254850905\n",
            "Episode finished after 495 timesteps .Reward:  199.5622514090789\n",
            "Episode finished after 219 timesteps .Reward:  230.23015699397402\n",
            "Episode finished after 308 timesteps .Reward:  229.800924026298\n",
            "Episode finished after 192 timesteps .Reward:  219.18580547475423\n",
            "Episode finished after 216 timesteps .Reward:  264.36335868571905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}