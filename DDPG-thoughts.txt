could it be that because im filling the replay buffer with 1,000,000 random actions, the network doesnt learn
anything fast enough because its repeatedly looking at bad experiences??? no? yeah?
Cause if you calculate the steps per episode for lunar landar for instance, its about 500 avg or 1000 max.
That means for 600 episodes, we are only ever going to fill the memory with 600,000 steps.

-i think i see why filling the replay memory with 1 milli random actions would cause training to be unstable.

-training significantly improved when we initalized memory with just batch size
-As we're training rn, critic loss function seems to be close hanging close to 0 with rare spikes in the 100's. 
this is important because its a mse which has 0 has "No Loss".

-followed up with doing random actions for 5000 in replay memory and started optimizing at 1000 as per open AI's implementation(exploration trick)

