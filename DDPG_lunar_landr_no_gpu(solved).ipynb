{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDPG_lunar_landr-no gpu.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "urEHsW7fpKX4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea7ce49-4ea5-4ec6-eb04-27f8400c4005"
      },
      "source": [
        "#!pip install box2d\n",
        "#!pip install wandb -q\n",
        "\n",
        "import wandb\n",
        "import torch     \n",
        "from torch import Tensor        \n",
        "import torch.autograd as autograd           \n",
        "import torch.nn as nn                   \n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init        \n",
        "import torch.optim as optim      \n",
        "from torch.distributions import Categorical         \n",
        "import gym\n",
        "import numpy as np\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from collections import deque\n",
        "import collections, itertools"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "ulyfXJuA7Sxa",
        "outputId": "86cadb20-42d6-4bbc-fff0-03a7f46c4c4d"
      },
      "source": [
        "wandb.login()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
            "wandb: Currently logged in as: olayemiy (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oz4sM6_U6JU"
      },
      "source": [
        "def init_fanin(tensor): #check for fan-in DDPG paper 7. experiment details\r\n",
        "  fanin = tensor.size(1)\r\n",
        "  v = 1.0 / np.sqrt(fanin)\r\n",
        "  init.uniform_(tensor, -v, v)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNHbnNtzpUpL"
      },
      "source": [
        " class DDPG_AC(nn.Module):\n",
        "   def __init__(self, obs_size, num_actions, linear_dim1, linear_dim2): \n",
        "     super(DDPG_AC,self).__init__()   \n",
        "     self.actor_fc1= nn.Linear(obs_size, linear_dim1)\n",
        "     init_fanin(self.actor_fc1.weight)\n",
        "     self.bn1 = nn.LayerNorm(linear_dim1)\n",
        "\n",
        "     self.actor_fc2 = nn.Linear(linear_dim1,linear_dim2)\n",
        "     init_fanin(self.actor_fc2.weight)\n",
        "     self.bn2 = nn.LayerNorm(linear_dim2)\n",
        "\n",
        "     self.actor_fc3 = nn.Linear(linear_dim2,num_actions)\n",
        "     init.uniform_(self.actor_fc3.weight, -3e-3, 3e-3)\n",
        "     init.uniform_(self.actor_fc3.bias, -3e-3, 3e-3)\n",
        "\n",
        "   def forward(self, obs):\n",
        "    actor = F.relu(self.bn1(self.actor_fc1(obs)))\n",
        "    actor = F.relu(self.bn2(self.actor_fc2(actor)))\n",
        "    actor = torch.tanh(self.actor_fc3(actor))\n",
        "\n",
        "    return actor\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJPIYbmQkyZk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4f3d42f9-9fb3-4252-98b5-2cdba174f87a"
      },
      "source": [
        "'''\n",
        "env = gym.make('LunarLanderContinuous-v2')\n",
        "obs = env.reset()\n",
        "print(obs)\n",
        "test = DDPG_AC(obs.shape[0],env.action_space.shape[0], 25).float() \n",
        "print(obs.shape)\n",
        "print(env.action_space.shape)\n",
        "print(test.forward(torch.tensor(obs).float()))\n",
        "'''\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nenv = gym.make('LunarLanderContinuous-v2')\\nobs = env.reset()\\nprint(obs)\\ntest = DDPG_AC(obs.shape[0],env.action_space.shape[0], 25).float() \\nprint(obs.shape)\\nprint(env.action_space.shape)\\nprint(test.forward(torch.tensor(obs).float()))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FuiC4SSFr85"
      },
      "source": [
        "class DDPG_CR(nn.Module):\n",
        "  def __init__(self, obs_size, num_actions, linear_dim1, linear_dim2): #linear_dim1 = 400\n",
        "    super(DDPG_CR,self).__init__()\n",
        "\n",
        "    self.critic_fc1 = nn.Linear(obs_size , linear_dim1)\n",
        "    init_fanin(self.critic_fc1.weight)\n",
        "    self.bn1 = nn.LayerNorm(linear_dim1)\n",
        "\n",
        "    self.critic_fc2 = nn.Linear(linear_dim1 + num_actions, linear_dim2) #inserting actions at second layer now\n",
        "    init_fanin(self.critic_fc2.weight)\n",
        "    self.bn2 = nn.LayerNorm(linear_dim2)\n",
        "\n",
        "    self.critic_fc3 = nn.Linear(linear_dim2, 1)\n",
        "    init.uniform_(self.critic_fc3.weight, -3e-3, 3e-3)\n",
        "    init.uniform_(self.critic_fc3.bias, -3e-3, 3e-3)\n",
        "\n",
        "  def forward(self, obs, action):\n",
        "    critic = F.relu(self.bn1(self.critic_fc1(obs)))\n",
        "    critic = torch.cat((critic, action), -1)\n",
        "    critic = F.relu(self.bn2(self.critic_fc2(critic)))\n",
        "    critic = self.critic_fc3(critic)\n",
        "\n",
        "    return critic #returns value for critic"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zu2RGUbtuE4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8c2b99fd-6f9d-49f2-bc58-622d02421b89"
      },
      "source": [
        "'''\n",
        "env = gym.make('LunarLanderContinuous-v2')\n",
        "obs = env.reset()\n",
        "test = DDPG_AC(obs.shape[0],env.action_space.shape[0], 25) \n",
        "cr = DDPG_CR(obs.shape[0],env.action_space.shape[0], 25) \n",
        "obs = torch.tensor(obs)\n",
        "action = test.forward(obs)\n",
        "print(obs.shape, action.shape)\n",
        "print(cr.forward(obs, action))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nenv = gym.make('LunarLanderContinuous-v2')\\nobs = env.reset()\\ntest = DDPG_AC(obs.shape[0],env.action_space.shape[0], 25) \\ncr = DDPG_CR(obs.shape[0],env.action_space.shape[0], 25) \\nobs = torch.tensor(obs)\\naction = test.forward(obs)\\nprint(obs.shape, action.shape)\\nprint(cr.forward(obs, action))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL3HU7pI6DhU"
      },
      "source": [
        "class OUActionNoise(object):\n",
        "    def __init__(self, mu, sigma=0.15, theta=.2, dt=1e-2, x0=None):\n",
        "        self.theta = theta\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.dt = dt\n",
        "        self.x0 = x0\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):#kinda like init but runs when you do noise().. if we created a noise object\n",
        "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
        "            self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
        "\n",
        "    def __repr__(self):#string representation of object\n",
        "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmUBidw1bCN2"
      },
      "source": [
        "class Agent():\n",
        "  def __init__(self, ENV_NAME='LunarLanderContinuous-v2', LINEAR_SIZE1=400, LINEAR_SIZE2=300, REPLAY_SIZE=int(1e6), BATCH_SIZE=64, TAU=0.001, AC_LR=0.001, CR_LR=0.001): #increased replay_size to 1 milli form 10,000\n",
        "    self.GAMMA = 0.99\n",
        "    self.TAU = TAU   # that T looking symbol for updating target network  \n",
        "    self.batch_size = BATCH_SIZE\n",
        "\n",
        "    self.env = gym.make(ENV_NAME)\n",
        "    self.obs = self.env.reset()\n",
        "\n",
        "    self.noise = OUActionNoise(mu=np.zeros(self.env.action_space.shape[0]))\n",
        "\n",
        "    self.net_ac = DDPG_AC(self.obs.shape[0], self.env.action_space.shape[0], LINEAR_SIZE1, LINEAR_SIZE2).float()\n",
        "    self.net_cr = DDPG_CR(self.obs.shape[0], self.env.action_space.shape[0], LINEAR_SIZE1, LINEAR_SIZE2).float()\n",
        "\n",
        "    self.target_net_ac = DDPG_AC(self.obs.shape[0], self.env.action_space.shape[0], LINEAR_SIZE1, LINEAR_SIZE2).float()\n",
        "    self.target_net_cr = DDPG_CR(self.obs.shape[0], self.env.action_space.shape[0], LINEAR_SIZE1, LINEAR_SIZE2).float()\n",
        "\n",
        "    self.target_net_ac.load_state_dict(self.net_ac.state_dict())\n",
        "    self.target_net_cr.load_state_dict(self.net_cr.state_dict())\n",
        "\n",
        "    self.optimizer_ac = optim.Adam(self.net_ac.parameters(), lr=AC_LR)\n",
        "    self.optimizer_cr = optim.Adam(self.net_cr.parameters(), lr=CR_LR)\n",
        "\n",
        "    self.Experience = collections.namedtuple('Experience',['state', 'action', 'reward', 'done', 'state1'])\n",
        "    self.memory = collections.deque(maxlen = REPLAY_SIZE)\n",
        "    self.max_replay = REPLAY_SIZE\n",
        "\n",
        "  \n",
        "  def init_replay_buffer(self):\n",
        "    for i in range(5000): #insted of filing the replay buffer to 1e6, im going to try filling it only up to batch size\n",
        "      action = self.env.action_space.sample()\n",
        "      obs1, reward, done, info = self.env.step(action)\n",
        "      experience = self.Experience(self.obs, action, reward, done, obs1)\n",
        "      self.memory.append(experience)\n",
        "      self.obs = self.env.reset() if done else obs1\n",
        "      \n",
        "      if(i>=1000 and i%50==0): #update every 50 steps # not going to run\n",
        "        states, actions, rewards, dones, state1s = self.get_batch()\n",
        "        critic_loss, actor_loss = self.calc_loss_update(states, actions, rewards, dones, state1s)\n",
        "        self.update_target()\n",
        "        \n",
        "\n",
        "  def act(self, episode_step_count, max_episode_steps):\n",
        "    self.obs = torch.tensor(self.obs)\n",
        "    action = self.net_ac.forward(self.obs.float())\n",
        "\n",
        "    self.action_log = action[0].item()\n",
        "\n",
        "    action = action + torch.Tensor(self.noise())  #basically turned up the exploration cause shit wasnt learning. *3 for MountainCarContinuous-v0, idk if different for lunar lander\n",
        "\n",
        "    self.action_noised_log = action[0].item()\n",
        "\n",
        "    obs1, reward, done, info = self.env.step(action.detach().numpy())\n",
        "\n",
        "    if(episode_step_count >= max_episode_steps): #this way we reset the obs and take care of the reward func calculation\n",
        "      done = True\n",
        "    \n",
        "    experience = self.Experience(self.obs.detach().numpy(), action.detach().numpy(), reward, done, obs1)\n",
        "    \n",
        "    self.obs = self.env.reset() if done else obs1\n",
        "\n",
        "    return experience, reward, done  #returning done to count number of episodes\n",
        "\n",
        "  def store_in_batch(self, experience):\n",
        "    self.memory.append(experience)\n",
        "\n",
        "  def get_batch(self):\n",
        "    random_indexes = np.random.choice(len(self.memory), self.batch_size, replace = False) \n",
        "    \n",
        "    states, actions, rewards, dones, state1s = zip(* [self.memory[index] for index in random_indexes]) # returns a list for each category\n",
        "\n",
        "    states = torch.Tensor(np.array(states))           \n",
        "    actions = torch.Tensor(np.array(actions))               \n",
        "    rewards = torch.Tensor(rewards)      \n",
        "    dones = torch.Tensor(dones)       \n",
        "    state1s = torch.Tensor(state1s)\n",
        "\n",
        "    return states, actions, rewards, dones, state1s\n",
        "\n",
        "  def calc_loss_update(self, states, actions, rewards, dones, state1s):\n",
        "    target_actor = self.target_net_ac(state1s.float())\n",
        "    target_critic = self.target_net_cr(state1s.float(), target_actor.detach())\n",
        "    target_critic = torch.squeeze(target_critic)\n",
        "\n",
        "    critic = self.net_cr(states.float(), actions.float())\n",
        "    critic = torch.squeeze(critic)\n",
        "    \n",
        "    #y = rewards + (self.GAMMA * target_critic * dones) \n",
        "    y = torch.empty(self.batch_size)\n",
        "    for i in range(self.batch_size):\n",
        "      y[i] = rewards[i] + (self.GAMMA * target_critic[i] * (1-dones[i]))\n",
        "\n",
        "    #Critic Loss\n",
        "    critic_loss = F.mse_loss(y.detach(), critic.float())\n",
        "    \n",
        "    self.optimizer_cr.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    self.optimizer_cr.step()\n",
        "\n",
        "    #Actor Loss\n",
        "    actor_loss = -self.net_cr(states,self.net_ac(states)).mean()\n",
        "\n",
        "    self.optimizer_ac.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    self.optimizer_ac.step()\n",
        "\n",
        "    return critic_loss, actor_loss\n",
        "    \n",
        "  def update_target(self):\n",
        "    for target_param, param in zip(self.target_net_ac.parameters(), self.net_ac.parameters()):\n",
        "      target_param.data.copy_(self.TAU * param.data + (1 - self.TAU) * target_param.data)\n",
        "\n",
        "    for target_param, param in zip(self.target_net_cr.parameters(), self.net_cr.parameters()):\n",
        "      target_param.data.copy_(self.TAU * param.data + (1 - self.TAU) * target_param.data)\n",
        "\n",
        "  def close_agent(self):\n",
        "    self.env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DKsCaUWcSEh"
      },
      "source": [
        "def train():\n",
        "  agent = Agent()\n",
        "  agent.init_replay_buffer()\n",
        "  episode_count = 0\n",
        "  avg_reward = 0\n",
        "  avg_100 =  collections.deque(maxlen = 100)\n",
        "  episode_step_count = 0\n",
        "  max_episode_steps = 1000\n",
        "  \n",
        "  wandb.init(project=\"ddpg\")\n",
        "  wandb.watch(agent.net_ac, log =\"all\")\n",
        "  wandb.watch(agent.net_cr, log =\"all\")\n",
        "  \n",
        "  while (episode_count < 7000): #lol just use while true since we dont know when this bitch ass implementation is gonna converge\n",
        "    with torch.autograd.set_detect_anomaly(True):\n",
        "      experience, reward, done = agent.act(episode_step_count, max_episode_steps)\n",
        "      agent.store_in_batch(experience)\n",
        "      states, actions, rewards, dones, state1s = agent.get_batch()\n",
        "      critic_loss, actor_loss = agent.calc_loss_update(states, actions, rewards, dones, state1s)\n",
        "\n",
        "      agent.update_target()\n",
        "\n",
        "      avg_reward+=reward\n",
        "      episode_step_count += 1\n",
        "\n",
        "      if done:\n",
        "        avg_100.append(avg_reward)\n",
        "        trail = (sum(avg_100)/len(avg_100))\n",
        "        \n",
        "        wandb.log({\"Reward\": avg_reward,\n",
        "                   \"Average 100 Rewards\": trail,\n",
        "                  \"Actor Loss\": actor_loss,\n",
        "                  \"Critic Loss\": critic_loss,\n",
        "                  \"Action\": agent.action_log,\n",
        "                  \"Noise + Action\": agent.action_noised_log,\n",
        "                  \"Episode Step Count\": episode_step_count,\n",
        "                  \"Replay Memory Size\": len(agent.memory)})\n",
        "               \n",
        "        if(trail >= 200):\n",
        "          torch.save(agent.net_ac.state_dict(),'lunar_saved.pth')\n",
        "          agent.close_agent()\n",
        "          break\n",
        "        if(episode_count%20==0):\n",
        "          torch.save(agent.net_ac.state_dict(),'lunar_saved.pth')\n",
        "\n",
        "        episode_count+=1\n",
        "        episode_step_count = 0\n",
        "        print('episode: ',episode_count, 'reward: ', avg_reward)\n",
        "\n",
        "        avg_reward = 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBMtjRHrdObd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "008e56c2-d17c-4095-f3d6-d6c4527a5e07"
      },
      "source": [
        "train()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33molayemiy\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.12<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">proud-tree-45</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/olayemiy/ddpg\" target=\"_blank\">https://wandb.ai/olayemiy/ddpg</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/olayemiy/ddpg/runs/m4qsu3yv\" target=\"_blank\">https://wandb.ai/olayemiy/ddpg/runs/m4qsu3yv</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20201216_200213-m4qsu3yv</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "episode:  1 reward:  -734.8383805142325\n",
            "episode:  2 reward:  -1160.857249845829\n",
            "episode:  3 reward:  -1141.2303304683342\n",
            "episode:  4 reward:  -577.081478154832\n",
            "episode:  5 reward:  -1268.9980017015998\n",
            "episode:  6 reward:  -1221.9929946269672\n",
            "episode:  7 reward:  -1073.4173879534414\n",
            "episode:  8 reward:  -468.1014564647585\n",
            "episode:  9 reward:  -455.1771287013089\n",
            "episode:  10 reward:  -483.8726421532127\n",
            "episode:  11 reward:  -813.3322317873767\n",
            "episode:  12 reward:  -681.1875754788256\n",
            "episode:  13 reward:  -904.4723651416334\n",
            "episode:  14 reward:  -583.6041791546422\n",
            "episode:  15 reward:  -412.0855159719436\n",
            "episode:  16 reward:  -328.84452742125984\n",
            "episode:  17 reward:  -1055.0195683651368\n",
            "episode:  18 reward:  -158.078724882326\n",
            "episode:  19 reward:  -241.55167662014622\n",
            "episode:  20 reward:  -115.29407536102138\n",
            "episode:  21 reward:  -106.43071561003123\n",
            "episode:  22 reward:  -138.72689989226689\n",
            "episode:  23 reward:  -127.0810896248788\n",
            "episode:  24 reward:  -241.5872518886905\n",
            "episode:  25 reward:  -214.70513883907282\n",
            "episode:  26 reward:  -186.896741071066\n",
            "episode:  27 reward:  -310.9747437661644\n",
            "episode:  28 reward:  -113.7281834526741\n",
            "episode:  29 reward:  -236.06692729276455\n",
            "episode:  30 reward:  -228.84665149078694\n",
            "episode:  31 reward:  -276.261762586851\n",
            "episode:  32 reward:  -125.63589146735464\n",
            "episode:  33 reward:  -263.34578249319907\n",
            "episode:  34 reward:  -104.16618385948317\n",
            "episode:  35 reward:  -141.8100492315366\n",
            "episode:  36 reward:  -242.23506975819237\n",
            "episode:  37 reward:  -99.1593711945344\n",
            "episode:  38 reward:  -107.8717607221055\n",
            "episode:  39 reward:  -99.79816761075949\n",
            "episode:  40 reward:  -188.99713867017073\n",
            "episode:  41 reward:  -207.3566234177448\n",
            "episode:  42 reward:  -188.79705420465643\n",
            "episode:  43 reward:  -106.70969827777805\n",
            "episode:  44 reward:  -119.93312906448934\n",
            "episode:  45 reward:  -82.88740284189542\n",
            "episode:  46 reward:  -194.44105142675681\n",
            "episode:  47 reward:  -284.98806955136513\n",
            "episode:  48 reward:  -238.19220803684775\n",
            "episode:  49 reward:  -107.20289895930333\n",
            "episode:  50 reward:  -57.049954871575245\n",
            "episode:  51 reward:  -223.52785925605212\n",
            "episode:  52 reward:  -86.44247895987789\n",
            "episode:  53 reward:  -127.15256037196097\n",
            "episode:  54 reward:  -101.76028447932552\n",
            "episode:  55 reward:  -136.06267681035632\n",
            "episode:  56 reward:  -118.54967237858818\n",
            "episode:  57 reward:  -164.11412100428237\n",
            "episode:  58 reward:  -145.89909563500913\n",
            "episode:  59 reward:  -131.93516210004412\n",
            "episode:  60 reward:  46.61272503688144\n",
            "episode:  61 reward:  295.03479054534296\n",
            "episode:  62 reward:  -49.80912256868688\n",
            "episode:  63 reward:  -76.45322563214422\n",
            "episode:  64 reward:  -138.16450192160835\n",
            "episode:  65 reward:  -160.2297385888809\n",
            "episode:  66 reward:  -146.13531619722863\n",
            "episode:  67 reward:  -104.99975880937103\n",
            "episode:  68 reward:  -191.33383039534016\n",
            "episode:  69 reward:  -99.5019505633424\n",
            "episode:  70 reward:  -89.62541592498586\n",
            "episode:  71 reward:  9.186121066655346\n",
            "episode:  72 reward:  -41.951110897485705\n",
            "episode:  73 reward:  -128.05456785301163\n",
            "episode:  74 reward:  -73.9364040341428\n",
            "episode:  75 reward:  -178.2440586952749\n",
            "episode:  76 reward:  -103.35137809833482\n",
            "episode:  77 reward:  -130.5832337820716\n",
            "episode:  78 reward:  -137.47407395715\n",
            "episode:  79 reward:  -219.84700144243516\n",
            "episode:  80 reward:  -42.56561429829083\n",
            "episode:  81 reward:  -83.21063222549438\n",
            "episode:  82 reward:  -48.087757191750484\n",
            "episode:  83 reward:  -206.35453052997116\n",
            "episode:  84 reward:  -248.04790702168322\n",
            "episode:  85 reward:  -149.1905653581863\n",
            "episode:  86 reward:  121.62349555829601\n",
            "episode:  87 reward:  208.98071501085093\n",
            "episode:  88 reward:  210.13366434136003\n",
            "episode:  89 reward:  -96.27164617928501\n",
            "episode:  90 reward:  210.84791597619636\n",
            "episode:  91 reward:  -49.5981772056513\n",
            "episode:  92 reward:  -115.87840592924172\n",
            "episode:  93 reward:  -92.11976091836435\n",
            "episode:  94 reward:  -99.61476355981576\n",
            "episode:  95 reward:  186.59389503418623\n",
            "episode:  96 reward:  111.20621689810926\n",
            "episode:  97 reward:  -18.13680641570484\n",
            "episode:  98 reward:  -30.03874827782937\n",
            "episode:  99 reward:  -37.72333218542919\n",
            "episode:  100 reward:  -95.91247362771789\n",
            "episode:  101 reward:  -185.78031248922957\n",
            "episode:  102 reward:  -20.753234677964144\n",
            "episode:  103 reward:  -32.56890301429263\n",
            "episode:  104 reward:  -209.69204882471166\n",
            "episode:  105 reward:  -209.1695635636222\n",
            "episode:  106 reward:  -92.3345727068315\n",
            "episode:  107 reward:  -8.92331902609607\n",
            "episode:  108 reward:  -193.94259350969955\n",
            "episode:  109 reward:  -25.1576375963881\n",
            "episode:  110 reward:  -16.699838351248715\n",
            "episode:  111 reward:  7.272561350434005\n",
            "episode:  112 reward:  -6.0309434941118445\n",
            "episode:  113 reward:  41.477168827066635\n",
            "episode:  114 reward:  -40.58877505599444\n",
            "episode:  115 reward:  -92.40507020436453\n",
            "episode:  116 reward:  -98.59686060548006\n",
            "episode:  117 reward:  -31.543300083117842\n",
            "episode:  118 reward:  -62.90199653321889\n",
            "episode:  119 reward:  -18.93586903465495\n",
            "episode:  120 reward:  -45.01693324562748\n",
            "episode:  121 reward:  -107.5212987808053\n",
            "episode:  122 reward:  -21.744752083156985\n",
            "episode:  123 reward:  6.642538461620066\n",
            "episode:  124 reward:  -62.737199798878486\n",
            "episode:  125 reward:  -28.547589475353405\n",
            "episode:  126 reward:  48.763491760868334\n",
            "episode:  127 reward:  -13.606638213033776\n",
            "episode:  128 reward:  -7.3708337861097935\n",
            "episode:  129 reward:  107.54438595374677\n",
            "episode:  130 reward:  -49.654685877811254\n",
            "episode:  131 reward:  -5.432329385760667\n",
            "episode:  132 reward:  -94.91796917575164\n",
            "episode:  133 reward:  -78.94545645349952\n",
            "episode:  134 reward:  -123.88454578314835\n",
            "episode:  135 reward:  -65.616476339499\n",
            "episode:  136 reward:  -41.21176845795768\n",
            "episode:  137 reward:  11.192868706904555\n",
            "episode:  138 reward:  160.27749094863867\n",
            "episode:  139 reward:  -25.154070662263976\n",
            "episode:  140 reward:  -35.520858991702895\n",
            "episode:  141 reward:  -90.21501511758164\n",
            "episode:  142 reward:  -158.31900239685314\n",
            "episode:  143 reward:  -63.54715759914811\n",
            "episode:  144 reward:  -50.19449919547788\n",
            "episode:  145 reward:  16.942201322310726\n",
            "episode:  146 reward:  -15.986037383536129\n",
            "episode:  147 reward:  -94.58720971570447\n",
            "episode:  148 reward:  -106.75648058293247\n",
            "episode:  149 reward:  4.930229262159263\n",
            "episode:  150 reward:  -35.048046790570076\n",
            "episode:  151 reward:  -156.38464012472403\n",
            "episode:  152 reward:  -87.25589813732756\n",
            "episode:  153 reward:  -47.76748577244101\n",
            "episode:  154 reward:  -275.8827168456909\n",
            "episode:  155 reward:  -48.8935092043234\n",
            "episode:  156 reward:  -40.769137995624824\n",
            "episode:  157 reward:  172.0046783183563\n",
            "episode:  158 reward:  -83.59784741971069\n",
            "episode:  159 reward:  164.03868804212698\n",
            "episode:  160 reward:  -125.06419254239617\n",
            "episode:  161 reward:  178.96098162280492\n",
            "episode:  162 reward:  185.15185088549077\n",
            "episode:  163 reward:  -47.06998951882498\n",
            "episode:  164 reward:  -91.46180925285506\n",
            "episode:  165 reward:  7.022811063897008\n",
            "episode:  166 reward:  -90.0776397699143\n",
            "episode:  167 reward:  195.43642673349905\n",
            "episode:  168 reward:  -70.88613569335281\n",
            "episode:  169 reward:  -51.179134762205486\n",
            "episode:  170 reward:  150.71571032928313\n",
            "episode:  171 reward:  -47.81335302473337\n",
            "episode:  172 reward:  -187.52908720709996\n",
            "episode:  173 reward:  191.29024081616888\n",
            "episode:  174 reward:  -10.613775139307144\n",
            "episode:  175 reward:  16.422737054242898\n",
            "episode:  176 reward:  224.00628704425463\n",
            "episode:  177 reward:  -33.644236141093636\n",
            "episode:  178 reward:  107.2406061724935\n",
            "episode:  179 reward:  131.91776347316258\n",
            "episode:  180 reward:  189.45060626499242\n",
            "episode:  181 reward:  200.548196506488\n",
            "episode:  182 reward:  -10.819232531892858\n",
            "episode:  183 reward:  -13.490829754089257\n",
            "episode:  184 reward:  1.6225023292031655\n",
            "episode:  185 reward:  -132.4297503411437\n",
            "episode:  186 reward:  137.07824361192286\n",
            "episode:  187 reward:  -17.08472470733141\n",
            "episode:  188 reward:  -49.92788339983726\n",
            "episode:  189 reward:  163.01000455614337\n",
            "episode:  190 reward:  -23.032454899093104\n",
            "episode:  191 reward:  222.92164198267452\n",
            "episode:  192 reward:  -105.58362717814683\n",
            "episode:  193 reward:  201.3196601395394\n",
            "episode:  194 reward:  72.08124727643597\n",
            "episode:  195 reward:  261.70772384254764\n",
            "episode:  196 reward:  -86.78851924304662\n",
            "episode:  197 reward:  -145.01617418417348\n",
            "episode:  198 reward:  -78.0874381477436\n",
            "episode:  199 reward:  191.8527275393397\n",
            "episode:  200 reward:  183.28149554648797\n",
            "episode:  201 reward:  216.85839131961177\n",
            "episode:  202 reward:  108.05775190637875\n",
            "episode:  203 reward:  -107.45624893844561\n",
            "episode:  204 reward:  -110.7298965598072\n",
            "episode:  205 reward:  -95.45611416193759\n",
            "episode:  206 reward:  -6.766053449061218\n",
            "episode:  207 reward:  -199.43460245078325\n",
            "episode:  208 reward:  220.7841185467497\n",
            "episode:  209 reward:  227.65232999275506\n",
            "episode:  210 reward:  258.77075568688286\n",
            "episode:  211 reward:  206.25215372778797\n",
            "episode:  212 reward:  23.885717620043906\n",
            "episode:  213 reward:  -40.31633059041183\n",
            "episode:  214 reward:  263.06681229045705\n",
            "episode:  215 reward:  238.8123385489572\n",
            "episode:  216 reward:  233.1132462338809\n",
            "episode:  217 reward:  6.024758102083\n",
            "episode:  218 reward:  210.4400826624423\n",
            "episode:  219 reward:  143.72187181113406\n",
            "episode:  220 reward:  128.85159562614874\n",
            "episode:  221 reward:  60.421056799291364\n",
            "episode:  222 reward:  250.2218316981346\n",
            "episode:  223 reward:  188.37326358246008\n",
            "episode:  224 reward:  215.4041613262303\n",
            "episode:  225 reward:  283.3366270807333\n",
            "episode:  226 reward:  237.55483874127395\n",
            "episode:  227 reward:  34.24093376142241\n",
            "episode:  228 reward:  216.27027686117952\n",
            "episode:  229 reward:  47.244089284870626\n",
            "episode:  230 reward:  38.20103392347596\n",
            "episode:  231 reward:  263.593180222117\n",
            "episode:  232 reward:  219.3611321718722\n",
            "episode:  233 reward:  258.659809858551\n",
            "episode:  234 reward:  276.61053312241114\n",
            "episode:  235 reward:  254.83870470524178\n",
            "episode:  236 reward:  266.94088365127334\n",
            "episode:  237 reward:  199.82574054997772\n",
            "episode:  238 reward:  210.72627793320868\n",
            "episode:  239 reward:  260.739546038925\n",
            "episode:  240 reward:  240.88069325208937\n",
            "episode:  241 reward:  250.5881215321288\n",
            "episode:  242 reward:  43.33605677205687\n",
            "episode:  243 reward:  125.60990688624914\n",
            "episode:  244 reward:  266.1110329335844\n",
            "episode:  245 reward:  167.32636882442443\n",
            "episode:  246 reward:  -13.076861220533615\n",
            "episode:  247 reward:  -25.872266231707652\n",
            "episode:  248 reward:  267.85252675357344\n",
            "episode:  249 reward:  249.05239044321505\n",
            "episode:  250 reward:  277.1799181275408\n",
            "episode:  251 reward:  255.949643938012\n",
            "episode:  252 reward:  12.014800837511416\n",
            "episode:  253 reward:  274.54943531116595\n",
            "episode:  254 reward:  195.29016593004553\n",
            "episode:  255 reward:  46.8540623371085\n",
            "episode:  256 reward:  64.8352988935876\n",
            "episode:  257 reward:  286.2096241977159\n",
            "episode:  258 reward:  208.94295027499643\n",
            "episode:  259 reward:  -6.067741292245302\n",
            "episode:  260 reward:  45.10266911934579\n",
            "episode:  261 reward:  267.3293810385713\n",
            "episode:  262 reward:  256.6435625436148\n",
            "episode:  263 reward:  254.70777454772505\n",
            "episode:  264 reward:  242.40250102347463\n",
            "episode:  265 reward:  259.0218085867968\n",
            "episode:  266 reward:  8.593886939589783\n",
            "episode:  267 reward:  257.72601317467496\n",
            "episode:  268 reward:  272.16188921798897\n",
            "episode:  269 reward:  267.1685184563869\n",
            "episode:  270 reward:  259.14835623476216\n",
            "episode:  271 reward:  254.2175077312273\n",
            "episode:  272 reward:  249.42638011083312\n",
            "episode:  273 reward:  -33.66763299585327\n",
            "episode:  274 reward:  276.73453191120416\n",
            "episode:  275 reward:  279.7370461452015\n",
            "episode:  276 reward:  25.375297770919573\n",
            "episode:  277 reward:  223.8563885999584\n",
            "episode:  278 reward:  270.9743925027774\n",
            "episode:  279 reward:  58.909084804610586\n",
            "episode:  280 reward:  253.65427665151444\n",
            "episode:  281 reward:  262.5080461960375\n",
            "episode:  282 reward:  266.8384331596502\n",
            "episode:  283 reward:  -66.84002636456069\n",
            "episode:  284 reward:  265.642661518597\n",
            "episode:  285 reward:  245.51606184900652\n",
            "episode:  286 reward:  176.07402208202737\n",
            "episode:  287 reward:  284.3609302740732\n",
            "episode:  288 reward:  31.110970717964562\n",
            "episode:  289 reward:  -174.26263832868366\n",
            "episode:  290 reward:  -117.13384137438265\n",
            "episode:  291 reward:  -148.03339418890747\n",
            "episode:  292 reward:  -117.6580568476914\n",
            "episode:  293 reward:  222.26353487677494\n",
            "episode:  294 reward:  194.4642778128964\n",
            "episode:  295 reward:  189.58774569087342\n",
            "episode:  296 reward:  272.0088108956864\n",
            "episode:  297 reward:  -41.83207014894786\n",
            "episode:  298 reward:  246.20169687558865\n",
            "episode:  299 reward:  261.2014652672961\n",
            "episode:  300 reward:  297.62819307368113\n",
            "episode:  301 reward:  257.4720286006955\n",
            "episode:  302 reward:  -11.733307960326954\n",
            "episode:  303 reward:  240.94502551287724\n",
            "episode:  304 reward:  -84.44008987707868\n",
            "episode:  305 reward:  153.85533017826592\n",
            "episode:  306 reward:  187.94413099823157\n",
            "episode:  307 reward:  226.08137233742147\n",
            "episode:  308 reward:  246.21548365727\n",
            "episode:  309 reward:  36.827028685132404\n",
            "episode:  310 reward:  56.35264371822103\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-e297199c5658>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_in_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate1s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m       \u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_loss_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate1s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-a985112dbe5f>\u001b[0m in \u001b[0;36mcalc_loss_update\u001b[0;34m(self, states, actions, rewards, dones, state1s)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m       \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGAMMA\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtarget_critic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m#Critic Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/traceback.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    358\u001b[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0;31m# If immediate lookup was desired, trigger lookups now.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/compilerop.py\u001b[0m in \u001b[0;36mcheck_linecache_ipython\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \"\"\"\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# First call the orignal checkcache as intended\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkcache_ori\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;31m# Then, update back the cache with our data, so that tracebacks related\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;31m# to our compiled codes can be produced.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/linecache.py\u001b[0m in \u001b[0;36mcheckcache\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mcontinue\u001b[0m   \u001b[0;31m# no-op for files loaded via a __loader__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mstat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3TUHVFHh755"
      },
      "source": [
        "\n",
        "'''\n",
        "#use to test model on computer\n",
        "import gym\n",
        "env = gym.make('LunarLanderContinuous-v2')\n",
        "observation = env.reset()\n",
        "net = DDPG_AC(observation.shape[0], env.action_space.shape[0], 400, 300).float()\n",
        "net.load_state_dict(torch.load('lunar_saved.pth'))\n",
        "net.eval()\n",
        "for i_episode in range(10):\n",
        "    observation = env.reset()\n",
        "    done = False\n",
        "    t = 0\n",
        "    total_r = 0\n",
        "    while not done:\n",
        "        env.render()\n",
        "        action = net(torch.tensor(observation).float())##\n",
        "        observation, reward, done, info = env.step(action.detach().numpy())\n",
        "        total_r+=reward\n",
        "        if done:\n",
        "            print(\"Episode finished after {} timesteps.\".format(t+1), \"Reward: \",total_r)\n",
        "            break\n",
        "        t+=1\n",
        "env.close()\n",
        "'''"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode finished after 342 timesteps .Reward:  205.76836661123698\n",
            "Episode finished after 1000 timesteps .Reward:  -45.21964558978234\n",
            "Episode finished after 219 timesteps .Reward:  193.13217254422935\n",
            "Episode finished after 261 timesteps .Reward:  203.09862346930754\n",
            "Episode finished after 453 timesteps .Reward:  251.6061254850905\n",
            "Episode finished after 495 timesteps .Reward:  199.5622514090789\n",
            "Episode finished after 219 timesteps .Reward:  230.23015699397402\n",
            "Episode finished after 308 timesteps .Reward:  229.800924026298\n",
            "Episode finished after 192 timesteps .Reward:  219.18580547475423\n",
            "Episode finished after 216 timesteps .Reward:  264.36335868571905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}